{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap Data for reviews on Jumia\n",
    "\n",
    "#### To Do\n",
    "- Search for a product on Jumia and manually check for results that have good amount of reviews.\n",
    "    - for this test, I will search for just 1 result with a good number of reviews\n",
    "- get the review url\n",
    "- use the url to scrap all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary Libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get product details\n",
    "def get_product_details(driver):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Extract product details\n",
    "    product_details = {\n",
    "        'product_name': soup.find('h1', class_='-fs20 -pts -pbxs').text.strip(),\n",
    "        # 'brand': soup.find('div', class_='-pvxs').find_all('a')[0].text.strip(),\n",
    "        'price': re.sub(r'[^\\d]', '', soup.find('span', class_='-b -ubpt -tal -fs24 -prxs').text.strip()),\n",
    "        # 'description': soup.find('div', class_='markup -mhm -pvl -oxa -sc').text.strip(),\n",
    "        'features': soup.find('div', class_ = 'markup -pam').text.strip(),\n",
    "        'specification': soup.find('ul', class_ = '-pvs -mvxs -phm -lsn').text.strip()\n",
    "    }\n",
    "\n",
    "    return product_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get the reviews on each page\n",
    "def get_page_review(driver):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    reviews = soup.find_all('article', class_='-pvs -hr _bet')\n",
    "\n",
    "    # Extract review data\n",
    "    review_data = []\n",
    "    for review in reviews:\n",
    "        \n",
    "        # Reviewer Header Comment\n",
    "        try:\n",
    "            reviewer_header_comment = review.find('h3', class_ = '-m -fs16 -pvs').text.strip()\n",
    "        except AttributeError:\n",
    "            reviewer_header_comment = ''\n",
    "        \n",
    "        # Reviewer Detail Comment\n",
    "        try:\n",
    "            reviewer_detail_comment = review.find('p', class_ = '-pvs').text.strip()\n",
    "        except AttributeError:\n",
    "            reviewer_detail_comment = ''\n",
    "        \n",
    "        # Review Dates\n",
    "        try:\n",
    "            review_date = review.find('span', class_ = '-prs').text\n",
    "        except AttributeError:\n",
    "            review_date = ''\n",
    "        \n",
    "        # Reviewer Name\n",
    "        try:\n",
    "            div = review.find(\"div\", class_=\"-df -j-bet -i-ctr -gy5\")\n",
    "            reviewer_name = div.find_all('span')[1].text.replace('by ', '')\n",
    "        except AttributeError:\n",
    "            reviewer_name = ''\n",
    "                \n",
    "        # product star\n",
    "        try:\n",
    "            product_star = review.find('div', class_ = 'stars _m _al -mvs').text\n",
    "        except AttributeError:\n",
    "            product_star = ''\n",
    "        \n",
    "        review_data.append({\n",
    "            'reviewer_name': reviewer_name,\n",
    "            'reviewer_header_comment': reviewer_header_comment,\n",
    "            'reviewer_detail_comment': reviewer_detail_comment,\n",
    "            'review_date': review_date,\n",
    "            'product_star': product_star\n",
    "        })\n",
    "    \n",
    "    return review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the scrap function\n",
    "def jumia_scrap(product_url):\n",
    "    # Clean product name incase it contains spaces\n",
    "    # product_name = product_name.replace(' ', '_')\n",
    "    \n",
    "    # Configure Selenium EdgeDriver options\n",
    "    options = Options()\n",
    "    options.use_chromium = True\n",
    "    options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\") # code to prevent the website from detecting bot activity\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\") # Disable the automation line at the top of the browser\n",
    "    options.add_argument('headless')\n",
    "    options.add_argument('disable-gpu')\n",
    "    service = Service(executable_path=r'Driver\\msedgedriver.exe') # Always check to confirm that the version of edge driver matches the version or MS Edge browser\n",
    "    driver = webdriver.Edge(service=service, options=options) # Initialize the webdriver\n",
    "\n",
    "    driver.get(product_url)\n",
    "    time.sleep(10) # To load the product page\n",
    "\n",
    "    # Get the product details\n",
    "    product_details = get_product_details(driver)\n",
    "    product_name = re.split(r\"[\\'\\\"]\", product_details['product_name'])[0]\n",
    "    \n",
    "    try:\n",
    "        review_page_button = driver.find_element(By.XPATH, '//a[@class=\"btn _def _ti -mhs -fsh0\"]')\n",
    "\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", review_page_button) # Scroll the page to view the button for Next Page\n",
    "        time.sleep(2)  # Wait for the scroll\n",
    "\n",
    "        review_page_button.click() # Clicks the next page button\n",
    "        time.sleep(10)  # Wait for the next page to load\n",
    "    except Exception as e:\n",
    "        print(\"There are no reviews for this product\")\n",
    "    \n",
    "    # Scrape all reviews - using a while loop\n",
    "    all_reviews = []\n",
    "\n",
    "    while True:\n",
    "        d_reviews = get_page_review(driver)\n",
    "        all_reviews.extend(d_reviews)\n",
    "\n",
    "        # Clicking the 'Next Page' button to get to other pages\n",
    "        try:\n",
    "            next_page_button = driver.find_element(By.XPATH, '//a[@aria-label=\"Next Page\"]')\n",
    "\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_page_button) # Scroll the page to view the button for Next Page\n",
    "            time.sleep(2)  # Wait for the scroll\n",
    "\n",
    "            next_page_button.click() # Clicks the next page button\n",
    "            time.sleep(10)  # Wait for the next page to load\n",
    "        except Exception as e:\n",
    "            print(\"No more pages to load\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Joing product details and the reviews\n",
    "    for review in all_reviews:\n",
    "        review.update(product_details)\n",
    "\n",
    "    # Save to CSV\n",
    "    product_df = pd.DataFrame(all_reviews)\n",
    "\n",
    "    output_dir = r'Reviews\\Jumia'\n",
    "    file_name = f'{product_name}_reviews.csv'\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    product_df.to_csv(output_path, index=False)\n",
    "    print(f'{len(product_df)} {product_name} reviews successfully written to {output_path}! Nice work!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages to load\n",
      "62 XIAOMI Redmi 14C 6.88 reviews successfully written to Reviews\\Jumia\\XIAOMI Redmi 14C 6.88_reviews.csv! Nice work!!!\n"
     ]
    }
   ],
   "source": [
    "# Update the start URL\n",
    "start_url = 'https://www.jumia.com.ng/xiaomi-redmi-14c-6.88-8gb-ram256gb-rom-android-12-sage-green-373375543.html'\n",
    "# 'https://www.jumia.com.ng/catalog/productratingsreviews/sku/AC431CL57R8EKNAFAMZ/' # small review data for testing\n",
    "# 'https://www.jumia.com.ng/catalog/productratingsreviews/sku/GE779EA1BY9XTNAFAMZ/' - more data\n",
    "# product_name = 'Wireless Rechargeable Mouse'\n",
    "\n",
    "jumia_scrap(start_url)\n",
    "\n",
    "# https://www.jumia.com.ng/itel-2163-wireless-fm-torchlight-dual-sim-black-84833750.html\n",
    "# https://www.jumia.com.ng/itel-p55-5g-6.6-hd-hole-6gb-ram128gb-rom-android-13-blue-274011937.html\n",
    "# https://www.jumia.com.ng/xiaomi-redmi-14c-6.88-8gb-ram256gb-rom-android-12-sage-green-373375543.html\n",
    "# https://www.jumia.com.ng/agm-note-n1-6.52-8gb-ram-128gb-expandable-rom-android-13-grey-235195482.html\n",
    "# https://www.jumia.com.ng/nokia-105african-edition-1.77-4mb4mb-800-mah-dual-sim-blue-132260276.html\n",
    "# https://www.jumia.com.ng/samsung-galaxy-a05-6.7-4gb-ram64gb-rom-android-13-black-277533765.html\n",
    "# https://www.jumia.com.ng/tecno-t101-1.8-dual-sim-black-382115813.html\n",
    "# https://www.jumia.com.ng/nokia-105african-edition1.774mb4mb800mah-dual-sim-charcoal-132260136.html\n",
    "# https://www.jumia.com.ng/xiaomi-redmi-14c-6.88-8gb-ram256gb-rom-android-12-sage-green-373375543.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
